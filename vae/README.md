# Variational AutoEncoders
This popular generative model involves the creation of an encoder and decoder network that translate images to latent space and latent space to images respectively. See here for more information: https://jaan.io/what-is-variational-autoencoder-vae-tutorial/ , https://danijar.com/building-variational-auto-encoders-in-tensorflow/

Here are some numbers generated by the vae: 
![alt text](https://github.com/Ashboy64/generative-models/blob/master/imgs/vae_mnist_generated_numbers.png)

After training, we can encode some of the data points to latent space and plot these points. This provides us with an interesting visualization of the MNIST dataset and how the autoencoder is learning its characteristics.
![alt text](https://github.com/Ashboy64/generative-models/blob/master/imgs/mnist_vae_visualization.png)
